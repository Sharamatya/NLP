{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zyJ25uz0kSaw"
   },
   "source": [
    "# Assignment 2 on Natural Language Processing\n",
    "\n",
    "### Date : 15th Sept, 2020\n",
    "\n",
    "### Instructor : Prof. Sudeshna Sarkar\n",
    "\n",
    "### Teaching Assistants : Alapan Kuila, Aniruddha Roy, Anusha Potnuru, Uppada Vishnu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ao1nhg9RknmF"
   },
   "source": [
    "The central idea of this assignment is to make you familiar with programming in python and also the language modelling task of natural language processing using the python.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "stk58juYkzEr"
   },
   "source": [
    "**Dataset:** \n",
    "\n",
    " Use the text file provided along."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Amatya\n",
      "[nltk_data]     Sharma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import re\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from pandas import *\n",
    "from nltk import LaplaceProbDist\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rT6byv49kdmo",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# read dataset\n",
    "f = open(\"corpus.txt\", \"r\", encoding=\"utf8\")\n",
    "corpa = f.read()\n",
    "f.close()\n",
    "# print(corpa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SRGqKaDn1pJy"
   },
   "source": [
    "Preprocess the data\n",
    "1. Here ngrams are to be calculated sentence wise meaning <wordA> <.> <wordB> is not a trigram. You are expected to tokenize corpus into a sentences and then find ngrams.\n",
    "2. We have processed the given corpus a bit such that each line in the text file contains a sentence or paragraph meaning no half sentence. If anyone is interested you can find the actual corpus at raw corpus.\t\n",
    "3. Some of the Preprocessing that needs to be done before calculating ngrams is as follows:\n",
    "\ta. If you observe corpus there are text like “_very_ much” which needs to be converted to “very much”.\n",
    "         b. Punctuations should be removed eg “how?” to \"how\"\n",
    "         c. Alphanumeric words may be removed \n",
    "         d. Converting all tokens to lower case to reduce vocab size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProc(sent):\n",
    "    sent = sent.lower()\n",
    "    sent = re.sub(r'[^a-zA-Z0-9\\s’]', ' ', sent)\n",
    "    sent = sent.replace(\"’\", \"\")\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Implementation Note :- \n",
    "Tasks done in preprocessing for each sentence::\n",
    "1. Replace all punctuations by a space.\n",
    "2. Remove every '’'. This implies wouldn't -> wouldnt.\n",
    "3. Convert to all to lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C1OtHn6B1oc2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpa_sents = sent_tokenize(corpa)\n",
    "vocab=[]\n",
    "\n",
    "for i in range(len(corpa_sents)):\n",
    "#     print(\"I/P::::: \",corpa_sents[i])\n",
    "    corpa_sents[i] = preProc(corpa_sents[i])\n",
    "#     print(\"O/P::::: \",corpa_sents[i])\n",
    "    vocab.extend(word_tokenize(corpa_sents[i]))\n",
    "\n",
    "vocab = list(set(vocab))\n",
    "# print(len(vocab))\n",
    "# print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YDL7yfpXkMRS"
   },
   "source": [
    "### Task: In this sub-task, you are expected to carry out the following tasks:\n",
    "\n",
    "1. **Create the following language models** on the training corpus: <br>\n",
    "    i.   Unigram <br>\n",
    "    ii.  Bigram <br>\n",
    "    iii. Trigram <br>\n",
    "    iv.  Fourgram <br>\n",
    "\n",
    "2. **List the top 5 bigrams, trigrams, four-grams (with and without Add-1 smoothing).**\n",
    "(Note: Please remove those which contain only articles, prepositions, determiners. For Example: “of the”, “in a”, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u3oIulBikPua"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Top 10 unigrams with stopwords w/o smoothing :\n",
      "          0     1\n",
      "0    (the,)  1643\n",
      "1    (and,)   872\n",
      "2     (to,)   729\n",
      "3      (a,)   632\n",
      "4    (she,)   541\n",
      "5     (it,)   530\n",
      "6     (of,)   514\n",
      "7   (said,)   462\n",
      "8      (i,)   410\n",
      "9  (alice,)   386\n",
      "\n",
      "\n",
      " Top 10 Bigrams with stopwords w/o smoothing :\n",
      "               0    1\n",
      "0    (said, the)  209\n",
      "1      (of, the)  133\n",
      "2  (said, alice)  116\n",
      "3        (in, a)   97\n",
      "4     (and, the)   82\n",
      "5      (in, the)   79\n",
      "6      (it, was)   76\n",
      "7      (to, the)   69\n",
      "8   (the, queen)   65\n",
      "9      (as, she)   61\n",
      "\n",
      "\n",
      " Top 10 Trigram with stopwords w/o smoothing :\n",
      "                          0   1\n",
      "0       (the, mock, turtle)  51\n",
      "1        (the, march, hare)  30\n",
      "2         (said, the, king)  29\n",
      "3      (the, white, rabbit)  21\n",
      "4       (said, the, hatter)  21\n",
      "5       (said, to, herself)  19\n",
      "6         (said, the, mock)  19\n",
      "7  (said, the, caterpillar)  18\n",
      "8           (she, went, on)  17\n",
      "9           (she, said, to)  17\n",
      "\n",
      "\n",
      " Top 10 Fourgrams w/o stopwords w/o smoothing :\n",
      "                           0   1\n",
      "0  (said, the, mock, turtle)  19\n",
      "1   (she, said, to, herself)  16\n",
      "2       (a, minute, or, two)  11\n",
      "3   (said, the, march, hare)   8\n",
      "4     (will, you, wont, you)   8\n",
      "5       (said, alice, in, a)   7\n",
      "6        (as, well, as, she)   6\n",
      "7     (well, as, she, could)   6\n",
      "8      (in, a, great, hurry)   6\n",
      "9          (in, a, tone, of)   6\n"
     ]
    }
   ],
   "source": [
    "unigrams=[]\n",
    "bigrams=[]\n",
    "trigrams=[]\n",
    "fourgrams=[]\n",
    "\n",
    "for content in (corpa_sents):\n",
    "    token_list = word_tokenize(content)\n",
    "    unigrams.extend(ngrams(token_list, 1))\n",
    "    bigrams.extend(ngrams(token_list, 2))\n",
    "    trigrams.extend(ngrams(token_list, 3))\n",
    "    fourgrams.extend(ngrams(token_list, 4))\n",
    "\n",
    "print(\"\\n Top 10 unigrams with stopwords w/o smoothing :\")\n",
    "f1 = nltk.FreqDist(unigrams)\n",
    "print(DataFrame(f1.most_common(10)))\n",
    "\n",
    "print(\"\\n\\n Top 10 Bigrams with stopwords w/o smoothing :\")\n",
    "f2 = nltk.FreqDist(bigrams)\n",
    "print(DataFrame(f2.most_common(10)))\n",
    "\n",
    "print(\"\\n\\n Top 10 Trigram with stopwords w/o smoothing :\")\n",
    "f3 = nltk.FreqDist(trigrams)\n",
    "print(DataFrame(f3.most_common(10)))\n",
    "\n",
    "print(\"\\n\\n Top 10 Fourgrams w/o stopwords w/o smoothing :\")\n",
    "f4 = nltk.FreqDist(fourgrams)\n",
    "print(DataFrame(f4.most_common(10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vARsvSfynePr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Top 10 unigrams w/o stopwords w/o smoothing :\n",
      "            0    1\n",
      "0     (said,)  462\n",
      "1    (alice,)  386\n",
      "2   (little,)  128\n",
      "3      (one,)  103\n",
      "4     (know,)   87\n",
      "5     (like,)   85\n",
      "6    (would,)   83\n",
      "7     (went,)   83\n",
      "8    (could,)   77\n",
      "9  (thought,)   74\n",
      "\n",
      "\n",
      " Top 10 Bigrams w/o stopwords w/o smoothing :\n",
      "                  0    1\n",
      "0     (said, alice)  116\n",
      "1    (mock, turtle)   54\n",
      "2     (march, hare)   31\n",
      "3  (thought, alice)   26\n",
      "4   (white, rabbit)   22\n",
      "5  (alice, thought)   12\n",
      "6     (poor, alice)   11\n",
      "7     (alice, said)   11\n",
      "8    (alice, could)   11\n",
      "9      (dont, know)   10\n",
      "\n",
      "\n",
      " Top 10 Trigram w/o stopwords w/o smoothing :\n",
      "                        0  1\n",
      "0   (poor, little, thing)  6\n",
      "1   (little, golden, key)  5\n",
      "2    (white, kid, gloves)  5\n",
      "3     (march, hare, said)  4\n",
      "4    (mock, turtle, said)  4\n",
      "5    (beau, ootiful, soo)  4\n",
      "6     (ootiful, soo, oop)  4\n",
      "7       (cats, eat, bats)  3\n",
      "8  (thought, poor, alice)  3\n",
      "9     (know, said, alice)  3\n",
      "\n",
      "\n",
      " Top 10 Fourgrams w/o stopwords w/o smoothing :\n",
      "                                       0  1\n",
      "0              (beau, ootiful, soo, oop)  4\n",
      "1             (e, e, evening, beautiful)  3\n",
      "2               (oh, dear, cried, alice)  2\n",
      "3         (mock, turtle, sighed, deeply)  2\n",
      "4     (e, evening, beautiful, beautiful)  2\n",
      "5  (evening, beautiful, beautiful, soup)  2\n",
      "6           (white, rabbit, blew, three)  2\n",
      "7          (rabbit, blew, three, blasts)  2\n",
      "8        (book, thought, alice, without)  1\n",
      "9    (thought, alice, without, pictures)  1\n"
     ]
    }
   ],
   "source": [
    "#stopwords = code for downloading stop words through nltk\n",
    "stpwords = stopwords.words('english')\n",
    "#print top 10 unigrams, bigrams after removing stopwords\n",
    "print(\"\\n Top 10 unigrams w/o stopwords w/o smoothing :\")\n",
    "uni_processed = [p for p in unigrams if not any(stop in p for stop in stpwords)]\n",
    "fdist1 = nltk.FreqDist(uni_processed)\n",
    "print(DataFrame(fdist1.most_common(10)))\n",
    "# print(fdist2.tabulate(10))\n",
    "#print top 10 bigrams, trigrams, fourgrams after removing stopwords\n",
    "print(\"\\n\\n Top 10 Bigrams w/o stopwords w/o smoothing :\")\n",
    "bi_processed = [p for p in bigrams if not any(stop in p for stop in stpwords)]\n",
    "fdist2 = nltk.FreqDist(bi_processed)\n",
    "print(DataFrame(fdist2.most_common(10)))\n",
    "\n",
    "print(\"\\n\\n Top 10 Trigram w/o stopwords w/o smoothing :\")\n",
    "tri_processed = [p for p in trigrams if not any(stop in p for stop in stpwords)]\n",
    "fdist3 = nltk.FreqDist(tri_processed)\n",
    "print(DataFrame(fdist3.most_common(10)))\n",
    "\n",
    "print(\"\\n\\n Top 10 Fourgrams w/o stopwords w/o smoothing :\")\n",
    "four_processed = [p for p in fourgrams if not any(stop in p for stop in stpwords)]\n",
    "fdist4 = nltk.FreqDist(four_processed)\n",
    "print(DataFrame(fdist4.most_common(10)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ioc1xNjmnim-"
   },
   "source": [
    "# Applying Smoothing\n",
    "\n",
    "\n",
    "Assume additional training data in which each possible N-gram occurs exactly once and adjust estimates.\n",
    "\n",
    "> ### $ Probability(ngram) = \\frac{Count(ngram)+1}{ N\\, +\\, V} $\n",
    "\n",
    "N: Total number of N-grams <br>\n",
    "V: Number of unique N-grams\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probNgram(fdist, ngram):\n",
    "    return (fdist[ngram]+1)/(fdist.N() + fdist.B())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add1 Implementation Notable Pts: :\n",
    "**Please note that STL function LaplaceProbDist() calculates same probability as user defined function probNgram. Although the STL function has <br>\n",
    "used to keep a single data struture for both frequency and probability distritbutions.**<br><br>\n",
    "    i.   Add-1/ Laplace Probability is directly propotional to frequency.<br>\n",
    "    ii.  Thus comparing frequencies instead of probabilities.<br>\n",
    "    iii. Reason being comparing natural numbers (freq) is more accurate than comparing double points (probs).<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "grh4sO0Yns4V"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Top 10 Bigrams with stopwords with smoothing :\n",
      "               0    1\n",
      "0    (said, the)  210\n",
      "1      (of, the)  134\n",
      "2  (said, alice)  117\n",
      "3        (in, a)   98\n",
      "4     (and, the)   83\n",
      "5      (in, the)   80\n",
      "6      (it, was)   77\n",
      "7      (to, the)   70\n",
      "8   (the, queen)   66\n",
      "9      (as, she)   62\n",
      "\n",
      "\n",
      " Top 10 Trigrams with stopwords with smoothing :\n",
      "                          0   1\n",
      "0       (the, mock, turtle)  52\n",
      "1        (the, march, hare)  31\n",
      "2         (said, the, king)  30\n",
      "3      (the, white, rabbit)  22\n",
      "4       (said, the, hatter)  22\n",
      "5       (said, to, herself)  20\n",
      "6         (said, the, mock)  20\n",
      "7  (said, the, caterpillar)  19\n",
      "8           (she, went, on)  18\n",
      "9           (she, said, to)  18\n",
      "\n",
      "\n",
      " Top 10 FourGrams with stopwords with smoothing :\n",
      "                           0   1\n",
      "0  (said, the, mock, turtle)  20\n",
      "1   (she, said, to, herself)  17\n",
      "2       (a, minute, or, two)  12\n",
      "3   (said, the, march, hare)   9\n",
      "4     (will, you, wont, you)   9\n",
      "5       (said, alice, in, a)   8\n",
      "6        (as, well, as, she)   7\n",
      "7     (well, as, she, could)   7\n",
      "8      (in, a, great, hurry)   7\n",
      "9          (in, a, tone, of)   7\n"
     ]
    }
   ],
   "source": [
    "#You are to perform Add-1 smoothing here:\n",
    "#write similar code for bigram, trigram and fourgrams\n",
    "fd_u = nltk.FreqDist(unigrams)\n",
    "\n",
    "fd_bi = nltk.FreqDist(bigrams)\n",
    "\n",
    "fd_tri = nltk.FreqDist(trigrams)\n",
    "\n",
    "fd_f = nltk.FreqDist(fourgrams)\n",
    "\n",
    "# n = len(vocab)\n",
    "add1_u = nltk.LaplaceProbDist(fd_u)\n",
    "\n",
    "add1_b = nltk.LaplaceProbDist(fd_bi)\n",
    "\n",
    "add1_t = nltk.LaplaceProbDist(fd_tri)\n",
    "            \n",
    "add1_f = nltk.LaplaceProbDist(fd_f)\n",
    "\n",
    "# for i in add1_f.samples():\n",
    "#     print(i[0:4], add1_f.prob(i))\n",
    "\n",
    "#Print top 10 unigram, bigram, trigram, fourgram after smoothing\n",
    "print(\"\\n\\n Top 10 Bigrams with stopwords with smoothing :\")\n",
    "df = DataFrame(add1_b.freqdist().most_common(10))\n",
    "df[1]+=1   # additional dataset\n",
    "# df[1] = add1_probrob(df[0])\n",
    "print(df)\n",
    "\n",
    "print(\"\\n\\n Top 10 Trigrams with stopwords with smoothing :\")\n",
    "df = DataFrame(add1_t.freqdist().most_common(10))\n",
    "df[1]+=1   # addional dataset \n",
    "print(df)\n",
    "\n",
    "print(\"\\n\\n Top 10 FourGrams with stopwords with smoothing :\")\n",
    "\n",
    "df = DataFrame(add1_f.freqdist().most_common(10))#, add1_f.freqdist().most_common(10))#, add1_f.prob(add1_f.freqdist().most_common(10)[0:3]))\n",
    "df[1]+=1\n",
    "\n",
    "# for i in df[0]:\n",
    "#      print(add1_f.prob(i), probNgram(fd_f, i))\n",
    "    \n",
    "        \n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k0GL40mQmmt4"
   },
   "source": [
    "### Predict the next word using statistical language modelling\n",
    "\n",
    "Using the above bigram, trigram, and fourgram models that you just experimented with, **predict the next word(top 5 probable) given the previous n(=2, 3, 4)-grams** for the sentences below.\n",
    "\n",
    "For str1, str2, you are to predict the next  2 possible word sequences using your trained smoothed models. <br> \n",
    "For example, for the string 'He looked very' the answers can be as below: \n",
    ">     (1) 'He looked very' *anxiouxly* \n",
    ">     (2) 'He looked very' *uncomfortable* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MBWKo5_Fmnbg"
   },
   "outputs": [],
   "source": [
    "str1 = 'after that alice said the'\n",
    "str2 = 'alice felt so desperate that she was'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Prediction Implementation Notes::\n",
    "We use two approaches for word prediction (in case of evaluation give preference to Approach 1)\"):<br>\n",
    "    \n",
    "1. **Approach 1 [predictNextAllModels()]**:<br>\n",
    "\n",
    "    i.  We calculate odds (conditional probabilities) for all words in vocab using all the models and output the top 5 words with max odds. <br>\n",
    "    ii. Intuitively its like listiing all the probabilities calculated from all the models in a 1-D array and print top 5 (unique) of them. <br>\n",
    "\n",
    "2. **Approach 2 [predictNextIndividualModels()]**: <br>\n",
    "    i. We calculate odds (conditional probabilities) for all words in the vocab considering each model and output the top 5 words with max odds w.r.t to the considered model. <br>\n",
    "    ii. Its like listiing all the probabilities calculated from a single model in a 1-D array and print top 5 (unique) of them. Repeat for all models.<br>\n",
    "\n",
    "Also it may appear tha o/p(Approach 1) = o/p(Approach 2 Bigram Model). But this is not the case always. Observe in the outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ext_nVn2mvZt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for string ' after that alice said the '  ::\n",
      "Top 5 Predictions All Models Combined :\n",
      "             0         1\n",
      "0         king  0.123134\n",
      "1       hatter  0.090298\n",
      "2         mock  0.082089\n",
      "3  caterpillar  0.077985\n",
      "4      gryphon  0.073880\n",
      "5      duchess  0.065671\n",
      "6          cat  0.061567\n",
      "7        queen  0.053358\n",
      "8        mouse  0.036940\n",
      "9        march  0.036940\n",
      "\n",
      "Top 5 Predictions Bigram Model : \n",
      "          0         1\n",
      "0     queen  0.029653\n",
      "1      king  0.027407\n",
      "2      mock  0.024262\n",
      "3   gryphon  0.024262\n",
      "4    hatter  0.023363\n",
      "5   duchess  0.017522\n",
      "6  dormouse  0.015725\n",
      "7     other  0.013928\n",
      "8     march  0.013928\n",
      "9     mouse  0.012580\n",
      "\n",
      "Top 5 Predictions Trigram Model : \n",
      "             0         1\n",
      "0         king  0.123134\n",
      "1       hatter  0.090298\n",
      "2         mock  0.082089\n",
      "3  caterpillar  0.077985\n",
      "4      gryphon  0.073880\n",
      "5      duchess  0.065671\n",
      "6          cat  0.061567\n",
      "7        queen  0.053358\n",
      "8        mouse  0.036940\n",
      "9        march  0.036940\n",
      "\n",
      "Top 5 Predictions QuadGram Model : \n",
      "              0         1\n",
      "0     recovered  0.986728\n",
      "1      soldiers  0.986728\n",
      "2         terms  0.986728\n",
      "3          jack  0.986728\n",
      "4        gloves  0.986728\n",
      "5        finger  0.986728\n",
      "6       opening  0.986728\n",
      "7  conversation  0.986728\n",
      "8         thick  0.986728\n",
      "9       history  0.986728\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Predictions for string ' alice felt so desperate that she was '  ::\n",
      "Top 5 Predictions All Models Combined :\n",
      "           0         1\n",
      "0        now  0.328909\n",
      "1      ready  0.219273\n",
      "2     losing  0.219273\n",
      "3    walking  0.219273\n",
      "4     dozing  0.219273\n",
      "5      quite  0.219273\n",
      "6         in  0.219273\n",
      "7  recovered  0.109636\n",
      "8   soldiers  0.109636\n",
      "9      terms  0.109636\n",
      "\n",
      "Top 5 Predictions Bigram Model : \n",
      "       0         1\n",
      "0      a  0.066023\n",
      "1    the  0.043327\n",
      "2    not  0.026822\n",
      "3  going  0.024759\n",
      "4   that  0.024759\n",
      "5   very  0.022695\n",
      "6     no  0.020632\n",
      "7     in  0.020632\n",
      "8  quite  0.018569\n",
      "9     so  0.016506\n",
      "\n",
      "Top 5 Predictions Trigram Model : \n",
      "             0         1\n",
      "0          now  0.107742\n",
      "1        quite  0.076959\n",
      "2            a  0.061567\n",
      "3      walking  0.046175\n",
      "4      looking  0.046175\n",
      "5    beginning  0.046175\n",
      "6        ready  0.030783\n",
      "7           so  0.030783\n",
      "8  considering  0.030783\n",
      "9       losing  0.030783\n",
      "\n",
      "Top 5 Predictions QuadGram Model : \n",
      "           0         1\n",
      "0        now  0.328909\n",
      "1      ready  0.219273\n",
      "2     losing  0.219273\n",
      "3    walking  0.219273\n",
      "4     dozing  0.219273\n",
      "5      quite  0.219273\n",
      "6         in  0.219273\n",
      "7  recovered  0.109636\n",
      "8   soldiers  0.109636\n",
      "9      terms  0.109636\n"
     ]
    }
   ],
   "source": [
    "#write code\n",
    "\n",
    "##################################################################################################\n",
    "# calculates and prints top 5 word predictions for each individual model \n",
    "# total outputs = 5.3 = 15\n",
    "def predictNextIndividualModels(strng):\n",
    "    tokens =  word_tokenize(strng)\n",
    "    retb = {}\n",
    "    rett = {}\n",
    "    ret4 = {}\n",
    "    n = len(tokens)\n",
    "    for word in vocab: #unigrams = vocab\n",
    "        retb[word] = 0.0\n",
    "        rett[word] = 0.0\n",
    "        ret4[word] = 0.0\n",
    "        if n >= 1:#predict with bigram model\n",
    "            retb[word] = min((add1_b.prob((tokens[n-1], word)))/(add1_u.prob((tokens[n-1],))), 1.0)\n",
    "            \n",
    "        if n >= 2:#predict with trigram model\n",
    "            rett[word] = min(1.0,(add1_t.prob((tokens[n-2], tokens[n-1], word)))/(add1_b.prob((tokens[n-2], tokens[n-1]))) )\n",
    "        if n >= 3:#predict with fourgram model\n",
    "            ret4[word] = min(1.0,(add1_f.prob((tokens[n-3], tokens[n-2], tokens[n-1], word)))/(add1_t.prob((tokens[n-3], tokens[n-2], tokens[n-1]))))                      \n",
    "\n",
    "        \n",
    "    k = Counter(retb)\n",
    "    print(\"\\nTop 5 Predictions Bigram Model : \" )\n",
    "    print( DataFrame(k.most_common(10)) )\n",
    "    \n",
    "    k = Counter(rett)\n",
    "    print(\"\\nTop 5 Predictions Trigram Model : \" )\n",
    "    print( DataFrame(k.most_common(10)) )\n",
    "    \n",
    "    k = Counter(ret4)\n",
    "    print(\"\\nTop 5 Predictions QuadGram Model : \" )\n",
    "    print( DataFrame(k.most_common(10)) )\n",
    "    \n",
    "    return\n",
    "\n",
    "##################################################################################################\n",
    "\n",
    "\n",
    "##################################################################################################\n",
    "# calculates and predicts most probable words using the combined use of all the models\n",
    "def predictNextAllModels(strng):\n",
    "    tokens =  word_tokenize(strng)\n",
    "    ret = {}\n",
    "    ret_wo_f ={}\n",
    "    n = len(tokens)\n",
    "    for word in vocab: #unigrams = vocab\n",
    "        ret[word]=0.0\n",
    "        ret_wo_f[word] = 0.0\n",
    "        p_w_bi = p_w_tri = p_w_f = -1.0\n",
    "        if n >= 1:#predict with bigram model\n",
    "            p_w_bi = (add1_b.prob((tokens[n-1], word)))/(add1_u.prob((tokens[n-1],)))\n",
    "\n",
    "        if n >= 2:#predict with trigram model\n",
    "            p_w_tri = (add1_t.prob((tokens[n-2], tokens[n-1], word)))/(add1_b.prob((tokens[n-2], tokens[n-1]))) \n",
    "\n",
    "        if n >= 3:#predict with fourgram model\n",
    "            p_w_f = (add1_f.prob((tokens[n-3], tokens[n-2], tokens[n-1], word)))/(add1_t.prob((tokens[n-3], tokens[n-2], tokens[n-1])))                      \n",
    "        \n",
    "        \n",
    "        ret[word] = max( min(ret[word],1), min(p_w_bi,1), min(p_w_tri,1), min(p_w_f,1) )\n",
    "        ret_wo_f[word] = max( min(ret_wo_f[word],1), min(p_w_bi,1), min(p_w_tri,1))\n",
    "        \n",
    "    k = Counter(ret)\n",
    "    df = DataFrame(k.most_common(10))\n",
    "    if max(df[1]) == min(df[1]) :\n",
    "        k = Counter(ret_wo_f)\n",
    "        df = DataFrame(k.most_common(10))  \n",
    "    return df\n",
    "\n",
    "##################################################################################################\n",
    "\n",
    "def print_output(strng):\n",
    "    print(\"Predictions for string '\",strng,\"'  ::\")\n",
    "    print(\"Top 5 Predictions All Models Combined :\" )\n",
    "    print(predictNextAllModels(strng))\n",
    "    predictNextIndividualModels(strng)\n",
    "    return\n",
    "\n",
    "\n",
    "str1 = preProc(str1)\n",
    "print_output(str1)\n",
    "\n",
    "print(\"\\n\\n\\n\")\n",
    "\n",
    "str2 = preProc(str2)\n",
    "print_output(str2)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NLP_Assignment_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
